{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Question 1 : Define Data Transformation in ETL and explain why it is important."
      ],
      "metadata": {
        "id": "RCu0bCicOl9f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Transformation is the second phase of the ETL (Extract, Transform, Load) process. It involves converting raw data extracted from various source systems into a format or structure suitable for the target destination (usually a data warehouse).\n",
        "\n",
        "Why It Is Important:\n",
        "\n",
        "Ensures Data Quality: By scrubbing errors and inconsistencies, it prevents \"garbage in, garbage out,\" ensuring the final data is reliable.\n",
        "\n",
        "Enables Compatibility: Sources often use different formats (SQL, CSV, JSON). Transformation bridges these gaps so systems can \"talk\" to each other.\n",
        "\n",
        "Improves Query Performance: Aggregating and structuring data beforehand reduces the computational load on the end-user when they run reports.\n",
        "\n",
        "Facilitates Better Decision-Making: By creating a \"single version of the truth,\" organizations can base their business intelligence on unified, accurate metrics."
      ],
      "metadata": {
        "id": "-Uq170uMPO3Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 2 : List any four common activities involved in Data Cleaning."
      ],
      "metadata": {
        "id": "55itdvhKPeQ6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here are four common activities involved in the process:\n",
        "\n",
        "\n",
        "\n",
        "1.   Removing Duplicates: Identifying and deleting identical records that may have been collected from multiple sources or entered twice. This prevents skewed metrics and double-counting.\n",
        "2.   Handling Missing Values: Deciding how to deal with \"null\" or empty data points. This is usually done through imputation (filling in a value based on statistical averages) or by removing the incomplete records entirely.\n",
        "\n",
        "3.   Standardization (Format Normalization): Ensuring all data follows the same format. For example, converting various date formats (e.g., \"12/01/2026\" and \"Jan 12, 26\") into a single standard like YYYY-MM-DD.\n",
        "4.   Outlier Detection: Identifying data points that fall significantly outside the expected range (e.g., a \"Customer Age\" of 250). These are either corrected or flagged as errors to maintain data integrity.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "kxzJeRYvPpR-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 3 : What is the difference between Normalization and Standardization?"
      ],
      "metadata": {
        "id": "SBcqqPKwQdkf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "While both Normalization and Standardization are data scaling techniques used during the ETL transformation phase to bring features onto a similar scale, they serve different mathematical purposes and are used in different scenarios.\n",
        "\n",
        "1. Normalization (Min-Max Scaling)\n",
        "Normalization rescales the data so that all values fall within a specific, fixed range‚Äîtypically 0 to 1 (or sometimes -1 to 1). It is highly sensitive to outliers because it uses the minimum and maximum values of the dataset\n",
        "\n",
        "\n",
        "*   The Goal: To ensure all features have the exact same scale, which is crucial for algorithms that do not assume a specific distribution (like Neural Networks or K-Nearest Neighbors).\n",
        "*   The Formula:\n",
        "x(norm)=x-x(min)/x(max)-x(min)\n",
        "\n",
        "2. Standardization (Z-Score Normalization)Standardization transforms data so that it has a mean ($\\mu$) of 0 and a standard deviation ($\\sigma$) of 1. Unlike normalization, it does not result in a fixed range (values can technically go to infinity), but it centers the data.\n",
        "\n",
        "\n",
        "\n",
        "*   The Goal: To make the data follow a standard normal distribution. This is preferred for algorithms that assume the data is Gaussian (like Linear Regression, Logistic Regression, or Linear Discriminant Analysis).\n",
        "*   The Formula:\n",
        "z=(x-ùùÅ)/ùúé\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "7Hw56EeIQlJs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 4 : A dataset has missing values in the ‚ÄúAge‚Äù column. Suggest two techniques to handle this and\n",
        "explain when they should be used."
      ],
      "metadata": {
        "id": "j4KxZgB3UigF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Handling missing data is a critical part of the ETL process. When dealing with an \"Age\" column, the approach you take can significantly impact the accuracy of your final analysis.\n",
        "\n",
        "Here are two common techniques for handling missing values:\n",
        "\n",
        "\n",
        "\n",
        "1.   1. Mean/Median Imputation\n",
        "This technique involves replacing the missing values with a calculated statistical value from the rest of the column.\n",
        "\n",
        "\n",
        "*   Mean Imputation: Use the average age for all missing entries.\n",
        "*   Median Imputation: Use the middle value of the age distribution.\n",
        "\n",
        "When to use it:\n",
        "\n",
        "\n",
        "\n",
        "*   Mean: Best when the data is normally distributed (symmetrical) and contains no extreme outliers.\n",
        "*   Median: Preferred when the data is skewed or contains outliers (e.g., if most people are 20-30, but there are a few 90-year-olds), as the median is more \"robust\" and won't be dragged up or down by extreme values.\n",
        "\n",
        "\n",
        "*   General Rule: Use this when the percentage of missing data is relatively small (typically under 5-10%).\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "2.   Listwise Deletion (Dropping Rows)\n",
        "This technique involves simply removing any record (row) that has a missing value in the \"Age\" column.\n",
        "\n",
        "When to use it:\n",
        "\n",
        "\n",
        "\n",
        "*   Missingness is Random: Use this if the data is \"Missing Completely at Random\" (MCAR), meaning there is no specific pattern to why the data is gone.\n",
        "*   Large Datasets: If you have 1 million rows and only 50 are missing the age, deleting them won't significantly impact your statistical power.\n",
        "\n",
        "\n",
        "*   When Accuracy is Paramount: If \"Age\" is the primary variable for your analysis (e.g., a study on age-related health), guessing the value through imputation might introduce too much \"noise\" or bias.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "UNE97wJ_Uxus"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 5 : Convert the following inconsistent ‚ÄúGender‚Äù entries into a standardized format (‚ÄúMale‚Äù, ‚ÄúFemale‚Äù):\n",
        "[\"M\", \"male\", \"F\", \"Female\", \"MALE\", \"f\"]"
      ],
      "metadata": {
        "id": "wIw1BHI5Xd_Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To standardize these entries, you need to map all variations of a value to a single, canonical format. In a real-world ETL pipeline, this is typically handled using a Mapping Dictionary or a CASE Statement in SQL to ensure consistency.\n",
        "\n",
        "Here is the standardized list:\n",
        "\n",
        "Standardized Output\n",
        "\n",
        "\n",
        "\n",
        "*   \"M\" - Male\n",
        "*   \"male\" - Male\n",
        "\n",
        "\n",
        "*   \"F\" - Female\n",
        "*   \"feame\" - Female\n",
        "\n",
        "\n",
        "*   \"MALE\" - Male\n",
        "*   \"f\" - Female\n",
        "\n",
        "Final Standardized Set: [\"Male\", \"Male\", \"Female\", \"Female\", \"Male\", \"Female\"]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "yIHEBWwHX0LL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Using SQL (CASE Statement):\n",
        "SELECT\n",
        "    CASE\n",
        "        WHEN UPPER(Gender) IN ('M', 'MALE') THEN 'Male'\n",
        "        WHEN UPPER(Gender) IN ('F', 'FEMALE') THEN 'Female'\n",
        "        ELSE 'Unknown'\n",
        "    END AS Standardized_Gender\n",
        "FROM raw_data_table;"
      ],
      "metadata": {
        "id": "9caeSkKLZvAZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 6 : What is One-Hot Encoding? Give an example with the categories: ‚ÄúRed, Blue, Green‚Äù."
      ],
      "metadata": {
        "id": "y-ISn101aCde"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "One-Hot Encoding is a transformation technique used to convert categorical data into a numerical format that machine learning algorithms can understand.\n",
        "\n",
        "Since most algorithms are based on mathematical equations, they cannot process text labels like colors or names directly. One-Hot Encoding creates a new binary column (containing only 0s or 1s) for every unique category in the original dataset.\n",
        "\n",
        "The Mechanism:\n",
        "\n",
        "\n",
        "\n",
        "*   Identify all unique labels in a column.\n",
        "\n",
        "*   Create a new column for each label.\n",
        "*   Assign a 1 to the column that matches the original label and a 0 to all other columns.\n",
        "\n",
        "Example: Colors\n",
        "\n",
        "If we have a dataset with the categories Red, Blue, and Green, the transformation would look like this:\n",
        "\n",
        "Original Data: | ID | Favorite Color | | :--- | :--- | | 1 | Red | | 2 | Blue | | 3 | Green | | 4 | Red |\n",
        "\n",
        "After One-Hot Encoding: | ID | Color_Red | Color_Blue | Color_Green | | :--- | :--- | :--- | :--- | | 1 | 1 | 0 | 0 | | 2 | 0 | 1 | 0 | | 3 | 0 | 0 | 1 | | 4 | 1 | 0 | 0 |\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "aiHyW9xhaF86"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 7 : Explain the difference between Data Integration and Data Mapping in ETL."
      ],
      "metadata": {
        "id": "15Tx_t_dbWDL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "While Data Integration and Data Mapping are closely related concepts in the ETL pipeline, they represent different levels of the process: one is the strategy/result, and the other is the technical blueprint.\n",
        "\n",
        "\n",
        "\n",
        "1.   Data Mapping (The Blueprint)\n",
        "Data mapping is the process of creating a connection between a specific field in one system and a specific field in another. It is the \"manual\" or \"rulebook\" that tells the ETL tool exactly where the data should go and how it should change.\n",
        "\n",
        "\n",
        "*   Focus: Field-to-field relationships.\n",
        "\n",
        "*   Key Questions: Does \"Cust_ID\" in Source A match \"Account_Number\" in Target B? Does the format need to change from a string to an integer?\n",
        "*   Deliverable: A mapping document or specification (often an Excel sheet or a visual diagram in an ETL tool).\n",
        "\n",
        "\n",
        "2.   Data Integration (The Result)\n",
        "\n",
        "\n",
        "\n",
        "*   Focus: The architectural goal and the final unified dataset.\n",
        "\n",
        "*   Key Questions: How do we combine CRM data, website logs, and financial records so the CEO can see a total overview of the business?\n",
        "*   Deliverable: A data warehouse, a unified database, or a real-time dashboard.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "fL67zfV6bZHh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 8 : Explain why Z-score Standardization is preferred over Min-Max Scaling when outliers exist.\n"
      ],
      "metadata": {
        "id": "cXTRbU3QctYQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The primary reason lies in how each method handles the boundaries of your data.\n",
        "\n",
        "1.   The \"Squeezing\" Problem in Min-Max Scaling\n",
        "\n",
        "Min-Max Scaling (Normalization) uses the absolute minimum and maximum values to define its range (usually 0 to 1). If you have a single outlier‚Äîfor example, a \"Salary\" column where most people earn $50,000 but one person earns $10,000,000‚Äîthat outlier becomes the \"1\".\n",
        "\n",
        "\n",
        "\n",
        "*   The Result: All the \"normal\" data points (the $50k earners) will be crushed into a tiny range like 0.0001 to 0.0005.\n",
        "\n",
        "*   The Impact: The algorithm loses the ability to distinguish between the regular data points because they have been \"squeezed\" so closely together to accommodate the extreme outlier.\n",
        "\n",
        "2. The \"Relative Distance\" Advantage of Z-Score\n",
        "\n",
        "Z-score Standardization does not have a fixed boundary. Instead, it scales data based on the Mean and the Standard Deviation.\n",
        "\n",
        "\n",
        "\n",
        "*   The Result: Outliers will still have high Z-scores (like 4 or 5), but they do not \"force\" the rest of the data into a microscopic range.\n",
        "*   The Impact: The relative distances and patterns between the majority of your data points are preserved. The outliers remain out on the \"fringes,\" but the core data remains spread out enough for a machine learning model to learn from it effectively\n",
        "\n"
      ],
      "metadata": {
        "id": "OpJEpgxvcxcI"
      }
    }
  ]
}